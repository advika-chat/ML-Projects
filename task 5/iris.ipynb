{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Train the decision tree\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Function to print the tree\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_tree\u001b[39m(node, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[1;32mIn[2], line 96\u001b[0m, in \u001b[0;36mbuild_tree\u001b[1;34m(X, y, depth, max_depth)\u001b[0m\n\u001b[0;32m     94\u001b[0m         node\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m thr\n\u001b[0;32m     95\u001b[0m         node\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m build_tree(X_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, max_depth)\n\u001b[1;32m---> 96\u001b[0m         node\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m, in \u001b[0;36mbuild_tree\u001b[1;34m(X, y, depth, max_depth)\u001b[0m\n\u001b[0;32m     80\u001b[0m node \u001b[38;5;241m=\u001b[39m TreeNode(\n\u001b[0;32m     81\u001b[0m     gini\u001b[38;5;241m=\u001b[39mgini_index(y),\n\u001b[0;32m     82\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(y),\n\u001b[0;32m     83\u001b[0m     num_samples_per_class\u001b[38;5;241m=\u001b[39mnum_samples_per_class,\n\u001b[0;32m     84\u001b[0m     predicted_class\u001b[38;5;241m=\u001b[39mpredicted_class,\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m max_depth:\n\u001b[1;32m---> 88\u001b[0m     idx, thr \u001b[38;5;241m=\u001b[39m \u001b[43mbest_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m         indices_left \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mvalues[:, idx] \u001b[38;5;241m<\u001b[39m thr\n",
      "Cell \u001b[1;32mIn[2], line 63\u001b[0m, in \u001b[0;36mbest_split\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, m):\n\u001b[0;32m     62\u001b[0m     c \u001b[38;5;241m=\u001b[39m classes[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mnum_left\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m     num_right[c] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m     gini_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28msum\u001b[39m((num_left[x] \u001b[38;5;241m/\u001b[39m i) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Iris.csv'\n",
    "iris_data = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare the data\n",
    "X = iris_data.drop(columns=['Id', 'Species'])\n",
    "y = iris_data['Species']\n",
    "\n",
    "# Manually encode the class labels as integers\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "y_encoded = np.array([class_mapping[label] for label in y])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded)\n",
    "\n",
    "# Define the structure of the tree node\n",
    "class TreeNode:\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "# Function to calculate the Gini index\n",
    "def gini_index(y):\n",
    "    m = len(y)\n",
    "    return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
    "\n",
    "# Function to find the best split\n",
    "def best_split(X, y):\n",
    "    m, n = X.shape\n",
    "    if m <= 1:\n",
    "        return None, None\n",
    "\n",
    "    num_classes = len(np.unique(y))\n",
    "    num_parent = [np.sum(y == c) for c in range(num_classes)]\n",
    "    best_gini = 1.0 - sum((num / m) ** 2 for num in num_parent)\n",
    "    best_idx, best_thr = None, None\n",
    "\n",
    "    for idx in range(n):\n",
    "        sorted_indices = np.argsort(X[:, idx])\n",
    "        thresholds, classes = X[sorted_indices, idx], y[sorted_indices]\n",
    "        num_left = [0] * num_classes\n",
    "        num_right = num_parent.copy()\n",
    "        for i in range(1, m):\n",
    "            c = classes[i - 1]\n",
    "            num_left[c] += 1\n",
    "            num_right[c] -= 1\n",
    "            gini_left = 1.0 - sum((num_left[x] / i) ** 2 for x in range(num_classes))\n",
    "            gini_right = 1.0 - sum((num_right[x] / (m - i)) ** 2 for x in range(num_classes))\n",
    "            gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "            if thresholds[i] == thresholds[i - 1]:\n",
    "                continue\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_idx = idx\n",
    "                best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "    return best_idx, best_thr\n",
    "\n",
    "# Recursive function to build the tree\n",
    "def build_tree(X, y, depth=0, max_depth=10):\n",
    "    num_samples_per_class = [np.sum(y == i) for i in range(len(np.unique(y)))]\n",
    "    predicted_class = np.argmax(num_samples_per_class)\n",
    "    node = TreeNode(\n",
    "        gini=gini_index(y),\n",
    "        num_samples=len(y),\n",
    "        num_samples_per_class=num_samples_per_class,\n",
    "        predicted_class=predicted_class,\n",
    "    )\n",
    "\n",
    "    if depth < max_depth:\n",
    "        idx, thr = best_split(X.values, y)\n",
    "        if idx is not None:\n",
    "            indices_left = X.values[:, idx] < thr\n",
    "            X_left, y_left = X[indices_left], y[indices_left]\n",
    "            X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "            node.feature_index = idx\n",
    "            node.threshold = thr\n",
    "            node.left = build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "            node.right = build_tree(X_right, y_right, depth + 1, max_depth)\n",
    "    return node\n",
    "\n",
    "# Train the decision tree\n",
    "tree = build_tree(X_train, y_train)\n",
    "\n",
    "# Function to print the tree\n",
    "def print_tree(node, depth=0):\n",
    "    if node is not None:\n",
    "        print(f\"{'|   ' * depth}Node(depth={depth}, gini={node.gini:.4f}, samples={node.num_samples}, value={node.num_samples_per_class}, class={node.predicted_class})\")\n",
    "        if node.left is not None or node.right is not None:\n",
    "            print(f\"{'|   ' * depth} feature_{node.feature_index} <= {node.threshold:.4f}\")\n",
    "            print_tree(node.left, depth + 1)\n",
    "            print(f\"{'|   ' * depth} feature_{node.feature_index} > {node.threshold:.4f}\")\n",
    "            print_tree(node.right, depth + 1)\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_tree(node, X):\n",
    "    if node.left is None and node.right is None:\n",
    "        return node.predicted_class\n",
    "    if X[node.feature_index] <= node.threshold:\n",
    "        return predict_tree(node.left, X)\n",
    "    else:\n",
    "        return predict_tree(node.right, X)\n",
    "\n",
    "# Print the tree structure\n",
    "print_tree(tree)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = [predict_tree(tree, x) for x in X_test.values]\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Function to plot the tree\n",
    "def plot_tree(node, depth=0, ax=None, pos=(0.5, 1), parent_pos=None, node_txt=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.axis('off')\n",
    "        \n",
    "    if node is not None:\n",
    "        x, y = pos\n",
    "        ax.text(x, y, f'Gini={node.gini:.4f}\\nSamples={node.num_samples}\\nClass={node.predicted_class}',\n",
    "                bbox=dict(facecolor='white', edgecolor='black'), ha='center')\n",
    "        if parent_pos is not None:\n",
    "            ax.plot([parent_pos[0], x], [parent_pos[1], y], 'k-')\n",
    "\n",
    "        if node.left is not None:\n",
    "            plot_tree(node.left, depth + 1, ax, (x - 0.5 / (depth + 1), y - 0.1), pos, '<=' + str(node.threshold))\n",
    "        if node.right is not None:\n",
    "            plot_tree(node.right, depth + 1, ax, (x + 0.5 / (depth + 1), y - 0.1), pos, '>' + str(node.threshold))\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plot_tree(tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
